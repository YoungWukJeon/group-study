# 주변 친구
- 앱 사용자 가운데 본인 위치 정보 접근 권한을 허락한 사용자에 핸해 인근의 친구 목록을 보여주는 시스템

## 문제 이해 및 설계 범위 확정
- 거리 : 5mile(8km) - 직선 거리 기준
- 이용자 수 : 10억, 기능 사용자 10%(약 1억명)
- 이동 이력 보관 여부 : O
- 비활성화 사용자 : 10분 이상 비활성화가 되는 경우 해당 사용자를 주변 친구 목록에서 제거
- 사생활 및 데이터 보호법 : 고려하지 않음

### 기능적 요구사항
- 모바일 앱을 통해 친구까지의 거리, 마지막 갱신 시간 표시
- 30초에 한번씩 갱신
- 사용자 한 명당 약 400명의 친구를 갖는다고 가정
- QPS : 10,000,000 / 30 = 334,000

### 비 기능적 요구사항
- 낮은 지연 시간 : 주변 친구의 위치 변화가 반영되는 데 걸리는 시간이 너무 오래 걸리지 않아야 한다.
- 안정성 : 몇 개의 데이터가 유실되는 것은 용인할 수 있음
- 결과적 일관성 : 위치 데이터를 저장하기 위해 강한 일관성을 지원하는 데이터 저장소를 사용할 필요는 없으며, 복제본의 데이터가 원본과 동일하게 변경되기까지 몇 초 정도 걸리는 것은 용인

## 개략적 설계안 제시 및 동의 구하기
- 근방의 활성 상태 친구의 위치 정보를 수신하고자 P2P 방식 사용 가능
- P2P를 사용하는 경우 통신 연결 상태, 전력 등의 이유로 인해 효율적이지 않으므로 공용 백엔드를 사용하는 방식으로 구성

#### 공용 백엔드의 역할
![2-3.png](images%2F2-3.png)
- 모든 활성 상태 사용자의 위치 변화 내역 수신
- 사용자 위치 변경 내역을 수신할 때마다 해당 사용자의 모든 활성 상태 친구를 찾아 친구들의 단말로 변경 내역 전송
- 두 사용자의 거리가 특정 임계치(5mile) 보다 먼 경우 내역을 전송하지 않음

##### 공용 백엔드 사용시 고려사항
- 큰 규모에 적용이 어려움
  - 주어진 문제의 가정은 활성 상태의 동시 접속 사용자가 1천만명(1억명의 10%)
  - 모두 자기 위치 정보를 30초 마다 갱신한다면 초당 334,000 번의 위치 정보 갱신을 처리해야함
  - 평균적으로 사용자 1명당 400명의 친구를 가지며 10%가 인근에 있다면 초당 334,000 * 400 * 10% = 1400만건의 정보 갱신 요청 처리 필요

### 설계안
![2-4.png](images%2F2-4.png)

#### 로드 밸런서
- RESTful API 서버 및 양방향 유상태 웹소켓 서버 앞단에 위치하며, 부하를 고르게 분산하기 위해 트래픽을 서버에 배분

#### RESTful API 서버
- 통상적인 요청/응답 트래픽 처리
- 친구 추가/삭제, 사용자 정보를 갱신하는 등의 부가적인 작업 처리

#### 웹소켓 서버
- 친구 위치 정보 변경을 거의 실시간에 가깝게 처리하는 유상태 서버 클러스터
- 클라이언트는 그 가운데 한 대와 웹소켓 연결을 지속적으로 유지
- 주변 친구 기능을 이용하는 클라이언트의 초기화도 담당
- 클라이언트가 시작되면 온라인 상태인 모든 주변 친구 위치를 해당 클라이언트로 전송하는 역할

#### 레디스 위치 정보 캐시
- 활성 상태 사용자의 가장 최근 위치 정보를 캐시
- TTL을 이용하여 기간이 지난 사용자는 비활성화 상태로 변경하고 캐시에서 삭제
- 캐시 정보가 갱신되는 경우 TTL도 갱신
- 레디스가 아닌 TTL을 지원하는 다른 키-값 저장소도 캐시로 활용 가능

#### 사용자 데이터베이스
- 사용자 데이터 및 사용자의 친구 관계 정보 저장
- RDB, NoSQL 모두 사용 가능

#### 위치 이동 이력 데이터베이스
- 사용자의 위치 변동 이력 보관

#### 레디스 pub/sub 서버
- 레디스 pub/sub은 초경량 메시지 버스
- 새로운 채널을 생성하는 것은 아주 *값싼 연산* GB급의 서버라면 수백만개의 채널 생성 가능
  - ![02-06.png](images%2F2-6.png)
- 웹소켓 서버를 통해 수신한 특정 사용자의 위치 정보 변경 이벤트를 사용자에게 배정된 pub/sub 채널에 발행
- 사용자의 친구는 해당 채널의 구독자
- 사용자의 위치가 변경되는 경우 모든 친구의 웹소켓 연결 핸들러가 호출되고, 수신할 친구가 활성 상태면 검색 반경 이내면 위치와 시간을 웹소켓으로 전달

#### 주기적 위치 갱신
- ![2-7.png](images%2F2-7.png)
- 항구적으로 유지되는 웹소켓 연결을 통해 주기적으로 위치 변경 내역 전송
```
1. 모바일 클라이언트가 위치가 변경된 사실을 로드밸런서에 전송
2. 로드밸런서는 위치 변경 내역을 해당 클라이언트와 웹소켓 서버 사이 설정된 연결을 통해 웹소켓 서버로 전송
3. 웹소켓 서버는 해당 이벤트를 위치 이동 이력 DB에 저장
4. 웹소켓 서버는 새 위치를 위치 정보 캐시에 보관, TTL 갱신, 웹소켓 연결 핸들러 안의 변수에 해당 위치 반영, 해당 변수는 이후 거리 계산 과정이 이용
5. 웹소켓 서버는 레디스 pub/sub 서버의 해당 사용자 채널에 새 위치를 발행 3~5는 병렬로 실행
6. 레디스 pub/sub 채널에 발행된 새로운 위치 변경 이벤트는 모든 구독자 웹소켓 이벤트 핸들러에 브로드캐스트되며 구독자는 위치 변경 이벤트를 보낸 사용자의 온라인 상태 친구들 각 구독자의 웹소켓 연결 핸들러는 친구의 위치 변경 이벤트를 수신
7. 메시지를 받은 웹소켓 서버는 새 위치를 보낸 사용자와 메시지를 받은 사용자 사이의 거리 계산
8. 7에서 계산한 거리가 검색 반경을 넘지 않는다면, 새 위치 및 해당 위치로의 이동이 발생한 시각을 나타내는 타임스탬프를 해당 구독자의 클라이언트 앱으로 전송하며, 검색 반경을 넘는 경우 전송하지 않음
```
위의 단계를 사례화 하면 아래와 같다.
- ![2-8.png](images%2F2-8.png)
아래의 과정이 해당 채널의 모든 구독자에게 반복 적용되며, 한 사용자당 평균 400명의 친구가 있으며, 약 10% 가량이 주변 온라인 상태로 가정하므로 약 40건 정도 발생
```
1. 사용자 1의 위치가 변경되면 변경 내역은 사용자 1과의 연결을 유지하고 있는 웹소켓 서버에 전송
2. 변경 내역이 레디스 pub/sub 서버 내의 사용자 1 전용 채널로 발행
3. 레디스 pub/sub 서버는 해당 변경 내역을 모든 구독자에게 브로드캐스트, 구독자는 사용자 1과 친구 관계에 있는 모든 웹소켓 연결 핸들러
4. 위치 변경 내역을 보낸 사용자와 구독자 사이의 거리가 검색 반경을 넘지 않을 경우새로운 위치 전송
```

### API 설계
웹소켓 : 사용자는 웹소켓 프로토콜을 통해 위치 정보 변경 내역을 전송하고 수신하기에 최소 API
HTTP 요청 : API 서버는 친구를 추가/삭제하거나 사용자 정보를 갱신하는 등의 작업을 처리, 여기서는 다루지 않음
1. [서버 API] 주기적인 위치 정보 갱신
- 요청 : 클라이언트는 위도, 경도, 시각 정보 전송
- 응답 : 없음
2. [클라이언트 API] 클라이언트가 갱신된 친구 위치를 수신하는데 사용할 API
- 전송되는 데이터 : 친구 위치 데이터와 변경된 시각을 나타내는 타임스탬프
3. [서버 API] 웹소켓 초기화 API
- 요청 : 클라이언트는 위도, 경도, 시각 정보를 전송
- 응담: 클라이언트는 자기 친구들의 위치 데이터 수신
4. [클라이언트 API] 새 친구 구독 API
- 요청 : 웹소켓 서버는 친구 ID 전송
- 응답 : 가장 최근의 위도, 경도, 시각 정보 전송
5. [클라이언트 API] 구독 해지 API
- 요청 : 웹소켓 서버는 친구 ID 전송
- 응답 : 없음

### 데이터 모델
사용자 DB는 개략적 설계안 참고, 해당 장에서는 위치 정보 캐시와 위치 이동 이력 DB에 대해서만 다룸

#### 위치 정보 캐시
- 주변 친구 기능을 켠 활성 상태 친구의 가장 최근 위치 정보 보관, 본 설계안에서는 레디스 캐시를 사용해 구현하며 아래와 같은 형태

|키|값|
|---|---|
|사용자 ID|{위도, 경도, 시각}|

#### 위치 정보 저장에 DB를 사용하지 않는 이유
- 주변 친구 기능은 현재 위치만을 사용하기에 하나만 보관해도 충분
- 레디스는 읽기 쓰기 연산 속도가 매우 빨라 이런 용도로 적합하며, TTL도 지원하여 더 이상 활성 상태가 아닌 사용자 정보를 자동으로 제거
- 주변 친구 기능의 위치 정보에 대해 영속성을 보장할 필요가 없음
- 레디스 서버에 장애가 발생하면, 새 서버로 바꾼 다음 갱신된 위치 정보가 캐시로 채워지면 정상화되며, 이 기간 동안 활성 상태 친구의 위치 변경 내역을 놓칠 수 있지만 앞서 상세 설계안에서 이에 대해 수용 가능한 것으로 지정

#### 위치 이동 이력 데이터베이스
위치 이동 이력 DB는 아래의 스키마 사용

|user_id|latitude|longitude|timestamp|
|---|---|---|---|

- 위치 이동 이력 DB는 막대한 쓰기 연산 부하를 감당할 수 있고, 수평적 규모 확장이 가능한 DB로 카산드라가 해당 요구사항에 적합
- RDB도 사용할 수 있으나, 이력 데이터의 양이 서버 한대에 보관하기에 너무 많을 수 있어 샤딩이 필요
- 사용자 ID를 기준으로 샤딩하는 것이 가장 기본적이며, 부하를 모든 샤드에 고르게 분산시키고, DB 운영 관리도 간편한 방법

## 상세 설계
### 중요 구성요소별 규모 확장성
#### API 서버
- RESTful API 서버의 규모 확장 방법은 알려진 방법이 많고, 해당 장에서 주로 다루는 내용이 아니기에 자세히 다루지 않음

#### 웹소켓 서버
- 웹소켓 서버의 경우 유상태 서버로 기존 서버를 제거하기 이전에 기존 연결을 종료하는 작업을 먼저 진행
- LB가 인식하는 노드 상태를 연결 종료 중(draining)으로 변경한 후 진행하며, 이렇게 진행하는 경우 새로운 웹소켓 연결이 생성되지 않음
- 모든 연결이 종료된 후 서버 제거 등의 작업을 진행
- 대부분의 클라우드 LB에서 이런 일을 잘 처리

#### 클라이언트 초기화
- 모바일 클라이언트는 시작 이후 웹소켓 클러스터 내의 서버와 지속적으로 웹소켓 연결을 맺음
- 지속성 연결의 이유는 연결이 오랜 시간 유지되기 때문인데, 현대적 프로그래밍 언어는 이런 연결 유지에 많은 메모리를 필요로 하지 않음
- 웹소켓 연결이 초기화되면 클라이언트는 해당 모바일 단말의 위치를 전송하며, 해당 데이터를 전송받은 웹소켓 연결 핸들러는 아래의 작업 진행

```
1. 위치 정보 캐시에 보관된 사용자의 위치 갱신
2. 해당 위치 정보는 뒤이은 계산 과정에 이용되므로, 연결 핸들러 내의 변수에 저장
3. 사용자 DB를 뒤져 해당 사용자의 모든 친구 정보를 가져옴
4. 위치 정보 캐시에 일괄(batch) 요청을 보내 모든 친구의 위치를 한번에 가져온다. 캐시에 보관하는 모든 항목의 TTL은 비활성화 타임아웃 시간과 동일한 값이므로 비활성화 친구는 캐시에 존재하지 않음
5. 캐시가 돌려준 친구 위치 각각에 대해 웹소켓 서버는 해당 친구와 사용자 사이의 거리를 계산하며, 거리가 검색 반경 이내이면 해당 친구의 상세 정보, 위치, 마지막으로 확인된 시각을 웹소켓 연결을 통해 클라이언트에 반환
6. 웹소켓 서버는 각 친구의 레디스 서버 pub/sub 채널을 구독하며, 채널 생성 및 구독 비용이 저렴하므로 사용자는 친구의 활설화/비활성화 상태에 관계없이 모든 친구 채널을 구독한다. 비활성화 상태 친구의 pub/sub 채널을 유지하기 위해 메모리가 필요한 것은 사실이지만 소량이며, 활성화 상태로 전환되기 전 CPU나 I/O를 전혀 이용하지 않는다.
7. 사용자의 현재 위치를 레디스 pub/sub 서버의 전용 채널을 통해 모든 친구에게 전송
```

#### 사용자 데이터베이스
- 사용자 데이터 베이스에 보관되는 데이터는 아래와 같음
  - 사용자 ID, 사용자명, 프로파일 이미지의 URL 등을 아우르는 사용자 상세 정보
  - 친구 관계 데이터
- 해당 데이터는 한 대의 RDB로는 감당할 수 없으며, 사용자 ID를 기준으로 샤딩을 통해 수평적 규모 확장 진행

#### 위치 정보 캐시
- 사용자 위치 정보 캐시는 레디스를 이용
- TTL이 적용되어 사용자의 위치 정보가 갱신될 때마다 초기화
- 최대 메모리 사용량은 일정 한도 아래로 유지
- 시스템이 가장 붐빌 때 천만 명의 사용자가 활성화 상태이며, 위치 정보 보관에 100Byte가 필요하다고 가정하는 경우 수 GB 이상의 메모리의 레디스 서버 한대로 모든 위치 정보 캐시 가능
- 단, 천만 명의 사용자가 30초마다 변경된 위치 정보를 전송한다면 레디스 서버는 초당 334K에 달하는 갱신 연산이 필요
- 캐시 데이터를 샤딩하여 사용자 ID를 기준으로 부하를 분산하는 방법 활용
- 가용성을 높이기 위해서는 각 샤드에 보관하는 위치 정보를 대기 노드에 복제하여 주 노드에 장애가 발생하는 경우 주 노드를 전환하여 장애시간을 줄이는 방안도 고려

#### 레디스 pub/sub 서버
- 해당 설계안에서 레디스 pub/sub 서버는 모든 온라인 친구에게 보내는 위치 변경 메세지의 라우팅 계층으로 활용
- 레디스 pub/sub을 사용하는 이유는 채널을 만드는 비용이 아주 저렴하기 때문
- 새 채널은 구독하려는 채널이 없을 때 생성
- 구독자가 없는 채널로 전송된 메세지는 그대로 삭제되며, 이때 서버의 부하는 거의 없음
- 채널 하나를 유지하기 위해 구독자 관계를 추적하기 위한 해시테이블과 연결리스트가 필요하지만 아주 소량의 메모리만 사용
- 오프라인 사용자라 어떤 변경도 없는 채널에 대해 CPU 자원은 거의 사용되지 않음
- 채널 유지에 많은 자원이 사용되지 않기 때문에 모든 사용자 별로 채널을 하나씩 생성하고 친구 관계의 모든 채널에 대한 구독 관계를 설정하며, 친구의 상태는 고려하지 않는다.
- 친구의 상태를 고려하지 않음으로 설계가 단순해지며, 활성화 상태로 변경되는 친구 채널을 구독하거나 비활성화 채널 구독을 취소하는 작업을 진행하지 않아도 된다.
- 다만, 메모리를 많이 사용할 수 있지만 메모리로 인해 병목되는 현상은 거의 발생하지 않는다.

#### 얼마나 많은 레디스 pub/sub 서버가 필요한가?
##### 메모리 사용량
- 주변 친구 기능을 사용하는 모든 사용자에게 채널을 하나씩 할당하는 경우 1억개
- 한 사용자의 활성화 상태 친구 가운데 100명이 주변 친구 기능을 사용한다면 해시 테이블과 연결 리스트에 20byte 상당의 포인터 => 약 200GB (100,000,000 * 20byte * 100) / 10^9
- 100GB 메모리의 서버 2대면 모든 채널을 보관할 수 있음

##### CPU 사용량
- pub/sub 서버가 구독자에게 전송해야하는 위치 정보 업데이트 양은 초당 약 14,000,000건이며, 보수적으로 현대적 서버 한대로 감당할 수 있는 구독자 수는 약 100,000이라고 가정하면 약 140대의 레디스 서버 필요
- 병목은 결국 메모리가 아닌 CPU에서 발생할 수 있으며, CPU 사용량에 대해 고려해야 한다.

#### 분산 레디스 pub/sub 클러스터
- 수백 대의 레디스 서버에 채널을 분산하려고 한다.
- 레디스 채널이 서로 독립적이기에 사용자 ID를 기준으로 pub/sub 서버를 샤딩하여 구현
- 수백대의 pub/sub 서버를 운영하기 위해서는 서버에 발생할 수 있는 장애 상황에 대한 대응법
- 서비스 탐색(service discovery) 컴포넌트를 도입하여 해결 가능하며, 아래의 기능을 사용
  - zookeeper, etcd 등 널리 사용되는 서비스 탐색 컴포넌트
  - 서비스 탐색 컴포넌트는 가용한 서버 목록을 유지하고, 해당 목록을 갱신하는데 필요한 UI나 API를 제공하는 키-값 저장소
    - ex) key : /config/pub_sub_ring , value : ["p_1", "p_2", "p_3", "p_4"]
  - 클라이언트로 하여금 value에 명시된 레디스 pub/sub 서버에서 발생한 변경 내역을 구독할 수 있는 기능
- 클라이언트가 구독해야할 pub/sub에 대해서는 해시 링 기법을 사용하여 결정
- ![2-9.png](images%2F2-9.png)
- 특정 사용자 채널에 위치 정보 변경 내역이 발행되는 경우 아래와 같은 순서로 처리
- ![2-10.png](images%2F2-10.png)
  1. 웹소켓은 해시 링을 참조하여 메세지를 발행할 레디스 pub/sub 서버를 선정하며, 서비스 탐색 컴포넌트를 활용하거나, 성능을 위해 해시 링 사본을 웹소켓 서버에 캐시하는 것도 가능
  2. 웹소켓 서버는 해당 서버가 관리하는 사용자 채널에 위치 정보 변경 내역 발행

#### 레디스 pub/sub 서버 클러스터의 규모 확장 고려사항
- 레디스 pub/sub 서버 클러스터의 규모를 늘린다면, 어떻게, 어떤 것을 기준으로 해야할까
  - 일반적인 경우 트래픽 패턴에 따라 클러스터 규모를 조정하며, 무상태 서버로 구성된 클러스터에 주로 사용되는 방식
- 레디스 pub/sub 서버는 무상태 서버 클러스터인가?
  - pub/sub 채널에 전송되는 메시지는 메모리나 디스크에 지속적으로 보관되지 않으며, 채널의 모든 구독자에게 전송된 후 삭제(만약 구독자가 없는 경우 바로 삭제) -> 무상태 서버
  - 하지만, pub/sub 서버는 채널에 대한 상태 정보를 보관하고 각 채널의 구독자 목록은 상태 정보의 핵심적 부분
  - 특정한 채널을 담당하던 pub/sub 서버를 교체하거나 해시 링에서 제거하는 경우 채널은 다른 서버로 이동시키고, 채널의 모든 구독자에게 그 사실을 알려야하며, 기존의 구독 관계를 끊고 새로운 구독 관계를 맺어야 한다. -> 유상태 서버
- 결과적으로 레디스 pub/sub 서버는 유상태 서버 클러스터라고 할 수 있음
- 유상태 서버 클러스터의 규모를 늘리거나 줄이는 것은 운영 부담과 위험이 큰 작업으로 주의 깊게 계획하고 진행이 필요
- 혼잡 시간대 트래픽을 무리 없이 감당하고 불필요한 크기 변화를 피할 수 있도록 오버 프로비저닝하는 것이 보통인데, 그럼에도 규모를 늘려야한다면 아래의 사항을 유의해야한다.
  - 클러스터 크기 조정시 많은 채널이 같은 해시 링 위의 다른 여러 서버로 이동하며 엄청난 재구독 요청이 발생
  - 재구독 요청에 대한 처리로 인해 위치 정보 변경 메시지 처리가 누락될 수 있으며, 이는 설계안에서 감안하지만 최소화시킬 필요가 있음
  - 서비스 상태가 불안정해질 가능성이 존재하므로 클러스터 크기 조정은 하루 중 부하가 가장 낮은 시간대에 진행
- 클러스터 크기 조정 방안
  - 새로운 링 크기를 계산하고 크기가 늘어나는 경우 새로운 서버를 준비
  - 해시 링의 키에 매달린 값을 새로운 내용으로 갱신
  - 모니터링을 통해 CPU 사용량이 어느정도인지 파악

#### 운영 고려사항
- 레디스 pub/sub 서버 교체시 고려 사항
  - 클러스터 크기를 조정할 때와 달리 채널의 대규모 이동 사태를 초래하지 않으며, 교체되는 서버의 채널만 영향을 미침
  - pub/sub 서버에 장애가 발생하면 on-call 엔지니어에게 경보를 발송하여 장애에 대응할 수 있도록 설정
  - on-call 엔지니어는 서비스 탐색 컴포넌트의 해시 링 키에 매달린 값을 갱신하여 장애가 발생한 노드를 대기중인 노드와 교체하는 작업을 진행
  - 웹소켓 서버는 이를 통해 새로운 노드를 통해 기존의 채널들을 구독하도록 변경
  - ![2-11.png](images%2F2-11.png)

#### 친구 추가/삭제
- 사용자가 친구를 추가하는 경우
  - 해당 클라이언트에 연결된 웹소켓 서버의 연결 핸들러에 그 사실을 알려 새 채널을 구독할 수 있도록 변경
  - 주변 친구 기능은 큰 앱의 일부 기능으로 새 친구 추가시 호출될 콜백을 앱에 등록하고, 콜백이 호출되는 경우 웹소케 서버로 새 친구의 pub/sub 채널을 구독하다는 메시지 전송
  - 웹소켓은 해당 친구가 활성화 상태인 경우 가장 최근 위치 및 시각 정보를 응답
- 사용자의 친구가 삭제되는 경우
  - 콜백을 통해 pub/sub 채널 구독 취소 메시지 전달

#### 친구가 많은 사용자
- 친구가 많은 사용자가 시스템 성능 문제를 야기할 수 있음
- 다만, 해당 케이스에서는 한 명이 맺을 수 있는 최대 친구 수에 상한이 있다고 가정하고, 친구는 양방향인 관계로 팔로어 모델 처럼 단방향의 관계가 아니기에 그럴 케이스는 없는 것으로 가정
- pub/sub 구독 관계는 클러스터 내 많은 웹소켓 서버에 분산되어 있어 친구들의 위치가 변경되는 데서 오는 부하가 나뉘어지므로, 핫스팟 문제는 발생하지 않을 것으로 가정

#### 주변의 임의 사용자
- 면접시 추가 점수를 얻을 수 있는 기본 설계안에는 존재하지 않는 내용
- 설계를 변경하여 정보 공유에 동의한 주변 사용자를 무작위로 보여줄 수 있도록 한다면 어떻게 해야할까
  - 1장 근접성 서비스에서 살펴본 지오해시를 기준으로 pub/sub채널을 생성하여 지원
  - 격자마다 pub/sub 채널이 있기 때문에 주변 격자 채널을 구독하고, 위치가 변경되는 경우에도 지오해시 ID채널과 인접 채널을 구독하여 위치를 전달받는 방식으로 구현
  - ![2-12.png](images%2F2-12.png)
  - ![2-13.png](images%2F2-13.png)ㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴ

#### 레디스 pub/sub 외의 대안
- 얼랭(Erlang)을 이용하여 대안책으로 사용 가능
  - 얼랭은 고도로 분산된 병렬 애플리케이션을 위해 고안된 프로그래밍 언어
  - 얼랭 프로세스 생성 비용은 리눅스 프로세스 생성 비용에 비해 엄청나게 저렴하며, 가장 작은 프로세스는 300byte, 아무일도 하지 않으면 CPU자원도 사용하지 않음

